{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L3T12_Intro_NN_CT1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a8YglsAarO23"
      },
      "source": [
        "# The power of neural networks\n",
        "\n",
        "Neurons can be used to model logic gates, the building blocks behind all digital computing. In this compulsory task we talk you through how to do so. We also explain how to represent neural networks in terms of matrix computations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nyr9_Tu6rO27",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7-mcTJnprO3I"
      },
      "source": [
        "## Neurons as logic gates\n",
        "\n",
        "A neuron works by applying an activation function, usually the sigmoid function, to a combination of inputs, input weights and a bias. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ft6TavXAWSS0",
        "colab": {}
      },
      "source": [
        "class Neuron():\n",
        "  \n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    \n",
        "  def activate(self, X):\n",
        "    return sigmoid(W * X + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H3pcg1o5XVuN"
      },
      "source": [
        "Here's a reminder of what the sigmoid function is and what it's output looks like:\n",
        "\n",
        "$$\n",
        "\\sigma = \\frac{1}{1 + e^{-x}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7WrC8RUprO3L",
        "outputId": "c6f1b304-b007-4732-8f69-ce88b1c37119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "  \n",
        "inputs = np.arange(-100,100, step=0.1)\n",
        "plt.plot(inputs, sigmoid(inputs), linewidth=2)\n",
        "plt.grid(True, which='both')\n",
        "plt.axhline(y=0, color='k')\n",
        "plt.axvline(x=0, color='k')\n",
        "plt.xlim([-10, 10])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-10, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzyYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AqAZ7aCSrO3Z"
      },
      "source": [
        "### Logic gates\n",
        "\n",
        "A logic gate takes in two boolean inputs (0 or 1, i.e. True or False) and returns a single boolean output. An OR gate, for example, returns a 1 if either of the inputs is 1 or both are 1, and 0 only if both inputs are 0. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B5fD9G7mX_vr"
      },
      "source": [
        "Can we design a neuron which produces the same outputs as an OR gate? \n",
        "\n",
        "In other words, can we find $w_1$, $w_2$ and $b$, such that $z$ in the following formula\n",
        "\n",
        "$$\n",
        "z = w_1 x_1 + w_2 x_2 + b\n",
        "$$\n",
        "\n",
        "corresponds to the outputs in the following OR gate truth table\n",
        "\n",
        "<table>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"3\">OR gate truth table</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"2\">Input</th>\n",
        "<th>Output</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jtJTGjH-Y7_9"
      },
      "source": [
        "It turns out that we can.\n",
        "To make it easier to understand how, let's tease apart the weights and inputs of our neuron class to allow 2 inputs and 2 weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6cCOeexuXejJ",
        "colab": {}
      },
      "source": [
        "class Neuron():\n",
        "  \n",
        "  def __init__(self, w1, w2, b):\n",
        "    self.w1 = w1\n",
        "    self.w2 = w2\n",
        "    self.b = b\n",
        "    \n",
        "  def activate(self, x1, x2):\n",
        "    return sigmoid(self.w1 * x1 + self.w2 * x2 + self.b)\n",
        " \n",
        "logic_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eF1cFmmWXts2"
      },
      "source": [
        "#### Bias\n",
        "\n",
        "The bias determines the value of $z$ if both inputs are 0.\n",
        "If both inputs are 0 we want the output to also be 0. So we must solve:\n",
        "\n",
        "$$\n",
        "0 =\\sigma(0 + 0 + b)\n",
        "$$\n",
        "\n",
        "\n",
        "The sigmoid function outputs values close to 0 if the input is about -7.5 or less, so $b$ must be at least that small. Let's specify $b$ to be -10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rxD_KxHgZo3-",
        "colab": {}
      },
      "source": [
        "b = -10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SyWWypZSZsYt"
      },
      "source": [
        "#### Weights\n",
        "\n",
        "The weights determine what happens when $x_1$ and/or $x_2$ are 1.\n",
        "In all the cases the output should be 1.\n",
        "\n",
        "The sigmoid function outputs about 1 for values larger than about 7.5, let's say 10. For either $w_1 + 0 + -10$ or $0 + w_1 + -10$ to be 10 or more, the weights must be at least 20. \n",
        "\n",
        "This also gives the correct output if both inputs are 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YX-JkugzbAek",
        "colab": {}
      },
      "source": [
        "w1, w2 = 20, 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o1nC-I1WbEhk"
      },
      "source": [
        "Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RWcOD1CFbFZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "65a6cdcd-b8da-4a6f-9971-8bc4a65a6347"
      },
      "source": [
        "def make_truth_table(gate):\n",
        "  for x1, x2 in logic_inputs:\n",
        "    output = gate.activate(x1, x2)\n",
        "    print(\"{}, {}: {}\".format(x1, x2, np.round(output)))\n",
        "\n",
        "or_gate = Neuron(w1, w1, b)\n",
        "make_truth_table(or_gate)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0, 0: 0.0\n",
            "0, 1: 1.0\n",
            "1, 0: 1.0\n",
            "1, 1: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ESl-j78arO3j"
      },
      "source": [
        "### Exercise\n",
        "Try to figure out what values for the neurons would model an AND gate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yKxZSnAQrO3i"
      },
      "source": [
        "<table>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"3\">AND gate truth table</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"2\">Input</th>\n",
        "<th>Output</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vgyObUnPrO3l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "86df6a48-27f2-484e-8431-85c7a40f46a0"
      },
      "source": [
        "# Fix this AND gate so its outputs are correct.\n",
        "\n",
        "and_gate = Neuron(w1=10, w2=10, b=-10)\n",
        "\n",
        "make_truth_table(and_gate)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0, 0: 0.0\n",
            "0, 1: 0.0\n",
            "1, 0: 0.0\n",
            "1, 1: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6TSxHQhOrO3o"
      },
      "source": [
        "### Exercise\n",
        "Do the same for the NOR gate and the NAND gate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j25DAnwvrO3p"
      },
      "source": [
        "<table>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"3\">NOR gate truth table</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"2\">Input</th>\n",
        "<th>Output</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z4mjHXntrO3p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b24ce3a1-742b-42e9-b349-d3b8b583e8a3"
      },
      "source": [
        "nor_gate = Neuron(w1=-10, w2=-10, b=10)\n",
        "\n",
        "make_truth_table(nor_gate)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0, 0: 1.0\n",
            "0, 1: 0.0\n",
            "1, 0: 0.0\n",
            "1, 1: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1IZ11iB5rO3r"
      },
      "source": [
        "<table>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"3\">NAND gate truth table</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"2\">Input</th>\n",
        "<th>Output</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iAeI8G44rO3s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "29a8e7d7-2821-4138-86de-db36223b461d"
      },
      "source": [
        "nand_gate = Neuron(w1=-5, w2=-5, b=10)\n",
        "\n",
        "make_truth_table(nand_gate)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0, 0: 1.0\n",
            "0, 1: 1.0\n",
            "1, 0: 1.0\n",
            "1, 1: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XwaXX4DmrO3w"
      },
      "source": [
        "### The XOR Gate\n",
        "\n",
        "Of all logic gates the most important in computer science is the exclusive or or XOR gate. \n",
        "\n",
        "It turns out there is no configuration for our neuron that will replicate the XOR gate truth table. \n",
        "\n",
        "However, the XOR can be modeled by combining three of the gates we just made.  In other words,\n",
        "by combining several neurons into a network.\n",
        "\n",
        "See if you can find the combination of gates that produces this table:\n",
        "\n",
        "<table>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"3\">XOR gate truth table</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<th colspan=\"2\">Input</th>\n",
        "<th>Output</th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "</tr>\n",
        "\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9q25E1v_ewLs"
      },
      "source": [
        "\n",
        "### Exercise\n",
        "\n",
        "Try to combine the gates we discussed. It's already if you do so by trial and error. To help you out, the code below specifies that our combination\n",
        "first passes the inputs to two separate hidden gates or hidden neurons, and then passes the outcome of that to a single output neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oe8f8HV1rO3x",
        "outputId": "5ef6d691-a7bb-40a6-da4a-2e04c7abb3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Uncomment the xor_gate line and find out which neurons besides the or_gate neuron the \n",
        "# network should have in its hidden and output layer to produce the right values.\n",
        "\n",
        "class Network():\n",
        "  \n",
        "  def __init__(self, gate1, gate2, out_gate):\n",
        "    self.hidden_neuron1 = gate1\n",
        "    self.hidden_neuron2 = gate2\n",
        "    self.out_neuron = out_gate\n",
        "   \n",
        "  def activate(self, x1, x2):\n",
        "    z1 = self.hidden_neuron1.activate(x1, x2)\n",
        "    z2 = self.hidden_neuron2.activate(x1, x2)\n",
        "    return self.out_neuron.activate(z1, z2)\n",
        "  \n",
        "xor_gate = Network(or_gate, and_gate, and_gate)\n",
        "make_truth_table(xor_gate)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0, 0: 0.0\n",
            "0, 1: 1.0\n",
            "1, 0: 1.0\n",
            "1, 1: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hg2jOJBArO30"
      },
      "source": [
        "## Matrix Computations\n",
        "\n",
        "The code for a single neuron is fairly simple. When we combine neurons, however, the input is passed through multiple neurons in a hidden layer, which can be very large. The output of the hidden layer is itself either passed to more layers or an output layer of variable size. This can involve absolutely huge computations which are hard to understand and code efficiently.\n",
        "\n",
        "To understand these computations and work with neural network libraries, you must refresh your linear algebra and be able to think of networks in terms of matrix calculations. We'll warm you up with this gentle exercise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6SELDEWQkcrq"
      },
      "source": [
        "### Input\n",
        "\n",
        "Instead of writing the input as seperate variables, we store each input as a vector and all inputs as a matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h35ap3pRrO36",
        "outputId": "66ab5ce3-3573-4461-e7d6-89f56a36fdd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "logic_inputs = np.array(logic_inputs)\n",
        "logic_inputs"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FpCii3ddk_nA"
      },
      "source": [
        "### Weights\n",
        "\n",
        "We do the same with weights.\n",
        "There are as many weight matrices as there are layers. \n",
        "Each cell $W_{i,j}$ in the matrix, where $i$ is the ith row and $j$ is the jth column, gives the weight from neuron $i$ in the previous (left) layer to neuron $j$ in the next (right) layer. In W, \n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WIV6j3talC9y",
        "outputId": "cca58238-2088-4c26-ca9d-ec98c74bc9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# weights of the hidden layer of an OR gate \n",
        "W = np.array([[20], \n",
        "              [20]])\n",
        "W"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[20],\n",
              "       [20]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C3AivjeisnPe"
      },
      "source": [
        "Instead of focusing on individual neurons, we focus on layers. \n",
        "We specify what size the input vectors ($m$) for the layer has, how many neurons ($n$) the layer has, and the bias for the layer. \n",
        "Instead of multipying each input with each neuron, we use np.dot to multiply the matrixes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xwmV63_msY2X",
        "outputId": "63b37b8e-1769-4816-eab2-df93136beb87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "class Layer():\n",
        "  \n",
        "  def __init__(self, W, b):\n",
        "    self.m = W.shape[0]\n",
        "    self.n = W.shape[1]\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    \n",
        "  def activate(self, X):\n",
        "    z = np.dot(X, self.W) + self.b\n",
        "    return sigmoid(z)\n",
        "\n",
        "OR_layer = Layer(W, -10)\n",
        "or_output = OR_layer.activate(logic_inputs)\n",
        "np.round(or_output)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LOV8eQhKwgCB"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Finish this version of an XOR gate that more closely resembles a neural network by determining the shapes the weights and biases need to have. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SfuZR6OCwo2l",
        "colab": {}
      },
      "source": [
        "W1 = np.array([[2,-1,2,3],[-1,1,-2,1],[3,-1,-1,3]])\n",
        "b1 = np.array([3,1,-2,1])\n",
        "\n",
        "W2 = np.array([[1,1,-1,1],[-2,1,1,-4],[-1,-3,2,-2],[3,1,2,1]])\n",
        "b2 = np.array([2,-1,1,4])\n",
        "\n",
        "hidden_layer = Layer(W1, b1)\n",
        "output_layer = Layer(W2, b2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rAs9CrJ7xG9_",
        "outputId": "87fc10df-0051-4146-cd86-177a7c0f4ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "class Network():\n",
        "  \n",
        "  def __init__(self, hidden, output):\n",
        "    self.hidden = hidden\n",
        "    self.output = output\n",
        "   \n",
        "  def activate(self, X):\n",
        "    z = self.hidden.activate(X)\n",
        "    return self.output.activate(z)\n",
        "\n",
        "xor_gate = Network(hidden_layer, output_layer)\n",
        "\n",
        "xor_output = xor_gate.activate(X)\n",
        "np.round(xor_output)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b2f3395d34ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mxor_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mxor_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxor_gate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxor_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    }
  ]
}